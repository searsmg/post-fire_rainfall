---
title: "Scrap code"
author: "Megan Sears"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: journal
editor_options:
  chunk_output_type: console
---

```{r, include = F}

knitr::opts_chunk$set(echo = F,
                      message = F,
                      fig.width = 12,
                      fig.height = 6)

library(tidyverse)
library(lubridate)
library(plotly)
library(kableExtra)
library(here)
library(ggplot2); theme_set(theme_bw(base_size = 16))

```

# Finish processing soil data

```{r}
# ETF
etf_soil <- read_csv('/Users/megansears/Documents/Repos/post-fire_rainfall/data/etf_soil.csv')

etf_soil_summary <- etf_soil %>%
  mutate(weight = shape_km2 / Area__km2) %>%
  group_by(Name) %>%
  summarize(ksat_wmean = weighted.mean(AVG_KSAT, weight, na.rm  = T),
            sand_wmean = weighted.mean(AVG_SAND, weight, na.rm  = T),
            silt_wmean = weighted.mean(AVG_SILT, weight, na.rm  = T),
            clay_wmean = weighted.mean(AVG_CLAY, weight, na.rm  = T))
         
  
write_csv(etf_soil_summary, './data/etf_soil_wmean.csv')

# Bennett
benn_soil <- read_csv('/Users/megansears/Documents/Repos/post-fire_rainfall/data/benn_soils_areaweight.csv')

benn_soil_summary <- benn_soil %>%
  mutate(weight = shape_km2 / total_area_km2) %>%
  group_by(FID_bennett_catchments_original6) %>%
  summarize(ksat_wmean = weighted.mean(AVG_KSAT, weight, na.rm  = T),
            sand_wmean = weighted.mean(AVG_SAND, weight, na.rm  = T),
            silt_wmean = weighted.mean(AVG_SILT, weight, na.rm  = T),
            clay_wmean = weighted.mean(AVG_CLAY, weight, na.rm  = T)) %>%
  arrange(FID_bennett_catchments_original6)

  
write_csv(benn_soil_summary, './data/benn_soil_wmean.csv')

```

# Extracting some precip time series for SK

```{r}

dates_to_filter <- c("2022-07-6", "2022-07-15", "2022-07-27")

all_2022 <- read_csv('/Users/megansears/Documents/MRMS/correctedSPR/2022/catchments_2022_mrms.csv') %>%
  dplyr::select(-1) %>%
  filter(date(datetime) %in% ymd(dates_to_filter),
         ID %in% c('me', 'mm', 'mw', 'ue', 'um', 'uw'))

looking <- ggplot(all_2022, aes(datetime, p_mm)) + geom_line()

ggplotly(looking)

write.csv(all_2022, 'mrms_july_2022_benn.csv')

```


## TB PIXEL ANAYLSIS

### 2021 

```{r extract tb pixels 2021}

# # unzip all grib2.gz (only do this once) **ALREADY COMPLETE
# zip <- list.files(path = '/Users/megansears/Documents/MRMS/QPE/2021', pattern = '.gz',
#                   full.names = T)
# 
# #unzip all listed files
# unzip <- lapply(zip, gunzip)

setwd('/Users/megansears/Documents/MRMS/MultiSensor/2021')

filenames <- list.files(".", pattern='.grib2', full.names=F)   

  for(fileName in filenames) {

#pull in raster
r <- rast(fileName) %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
crop <- terra::crop(r, ext(bbox))

names(crop) <- 'p_mmhr'

#extract using terra bc it has more functions :)
extract <- terra::extract(crop, rain)
extract$name <- rain$site

timestamp <- substr(fileName, 33, 47)

extract <- extract %>%
  mutate(datetime = ymd_hms(timestamp)) %>%
  mutate(datetime = datetime - (7 * 60 * 60)) %>%
  mutate(doy = yday(datetime)) %>%
  mutate(hour = hour(datetime))

write.csv(extract, paste0('extract_', extract$doy[1],'_', extract$hour[1],'.csv'))

  }

```

```{r get pixel csv 2021}

#compile all CSVs
# pixel_21 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2021', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# pixel_22 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2022', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# #remove random columns
# pixel_21 <- pixel_21 %>%
#   dplyr::select(-c(...1, ID))
# 
# pixel_22 <- pixel_22 %>%
#   dplyr::select(-c(...1, ID, doy, hour))
# 
# write_csv(pixel_21, 'mrms_pixel_21.csv')
# write_csv(pixel_22, 'mrms_pixel_22.csv')

```

## 2022

```{r extract tb pixels 2022}

setwd('/Users/megansears/Documents/MRMS/MultiSensor/2022')

filenames <- list.files(".", pattern='.grib2', full.names=F)   

  for(fileName in filenames) {

#pull in raster
r <- rast(fileName) %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
crop <- terra::crop(r, ext(bbox))

names(crop) <- 'p_mmhr'

#extract using terra bc it has more functions :)
extract <- terra::extract(crop, rain)
extract$name <- rain$site

timestamp <- substr(fileName, 33, 47)

extract <- extract %>%
  mutate(datetime = ymd_hms(timestamp)) %>%
  mutate(datetime = datetime - (7 * 60 * 60)) %>%
  mutate(doy = yday(datetime)) %>%
  mutate(hour = hour(datetime))

write.csv(extract, paste0('extract_', extract$doy[1],'_', extract$hour[1],'.csv'))

  }

```

```{r get pixel csv 2022}

#compile all CSVs
# pixel_21 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2021', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# pixel_22 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2022', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# #remove random columns
# pixel_21 <- pixel_21 %>%
#   dplyr::select(-c(...1, ID))
# 
# pixel_22 <- pixel_22 %>%
#   dplyr::select(-c(...1, ID, doy, hour))
# 
# write_csv(pixel_21, 'mrms_pixel_21.csv')
# write_csv(pixel_22, 'mrms_pixel_22.csv')

```

## WATERSHED MRMS

### Pull watershed MRMS
Using the multi sensor QPE 

```{r}
# do ET first 
setwd('/Users/megansears/Documents/MRMS/MultiSensor/2022')

filenames <- list.files(".", pattern='.grib2', full.names=F)   

  for(fileName in filenames) {

#pull in raster
r <- rast(fileName) %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
crop <- terra::crop(r, ext(bbox))

names(crop) <- 'p_mmhr'

#extract using terra bc it has more functions :)
extract <- terra::extract(crop, et_sheds, fun = mean, touches = T, exact = T)
extract$name <- et_sheds$Name

timestamp <- substr(fileName, 33, 47)

extract <- extract %>%
  mutate(datetime = ymd_hms(timestamp)) %>%
  mutate(datetime = datetime - (7 * 60 * 60)) %>%
  mutate(doy = yday(datetime)) %>%
  mutate(hour = hour(datetime))

write.csv(extract, paste0('extract_', extract$doy[1],'_', extract$hour[1],'.csv'))

  }

```

```{r get watershed csv 2021}

#compile all CSVs
etf_mrms_21 <- list.files(path='/Users/megansears/Documents/MRMS/MultiSensor/etf_2021', full.names = T) %>%
  lapply(read_csv) %>%
  bind_rows

etf_mrms_22 <- list.files(path='/Users/megansears/Documents/MRMS/MultiSensor/etf_2022', full.names = T) %>%
  lapply(read_csv) %>%
  bind_rows

#remove random columns
etf_mrms_21 <- etf_mrms_21 %>%
  dplyr::select(-c(...1, ID))

etf_mrms_22 <- etf_mrms_22 %>%
  dplyr::select(-c(...1, ID, doy, hour))

write_csv(etf_mrms_21, 'etf_mrms_21.csv')
write_csv(etf_mrms_22, 'etf_mrms_22.csv')

```

# MISC CODE

## doing a test of Dry Creek pixel for summer 2022

```{r}

dry_rain <- rain %>%
  filter(site == 'dry')

library(foreach)
library(doParallel)

# Set the number of cores for parallel processing
num_cores <- 8

# Set the working directory
setwd("/Users/megansears/Documents/MRMS/correctedSPR")

# Get the list of raster filenames
filenames <- list.files(".", pattern = ".tif", full.names = FALSE)

# Register parallel backend with the specified number of cores
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Define the output directory
output_dir <- "/Users/megansears/Documents/MRMS/correctedSPR/dry"

# Parallelize the loop using foreach
foreach(fileName = filenames, .packages = c("raster", "lubridate", "dplyr")) %dopar% {
  # Pull in raster
  r <- terra::rast(fileName)
  names(r) <- "p_mmhr"
  
  # Extract using terra package
  extract <- terra::extract(r, dry_rain)
  extract$name <- dry_rain$site
  
  timestamp <- substr(fileName, 12, 26)
  
  # Manipulate the extracted data
  extract <- extract %>%
    mutate(datetime = ymd_hms(timestamp)) %>%
    # mutate(datetime = datetime - (7 * 60 * 60)) %>%
    mutate(doy = yday(datetime)) %>%
    mutate(hour = hour(datetime),
           min = minute(datetime))
  
  # Generate the output filename
  output_filename <- paste0(output_dir, "extract_", extract$doy[1], "_", extract$hour[1], "_", extract$min[1], ".csv")
  
  # Write the extracted data to a CSV file
  write.csv(extract, file = output_filename)
  
  # Return a message indicating completion
  cat("Data extraction completed for:", fileName, "\n")
}

# Stop the parallel backend
stopCluster(cl)

##############################
# move the csvs into another folder
# Set the source and destination directory paths
source_dir <- '/Users/megansears/Documents/MRMS/correctedSPR'
destination_dir <- '/Users/megansears/Documents/MRMS/correctedSPR/dry'

# Get the list of files in the source directory that have 2022
file_list <- list.files(source_dir, pattern = '.csv')

# Move each file to the destination directory
for (file in file_list) {
  source_path <- file.path(source_dir, file)
  destination_path <- file.path(destination_dir, file)
  
  # Copy the file to the destination directory
  file.copy(source_path, destination_path)
  
  # Remove the original file from the source directory
  file.remove(source_path)
}

test<- read_csv('/Users/megansears/Documents/MRMS/correctedSPR/dry/dryextract_122_0_14.csv')

###########################################

# Define the directory where the CSV files are located
csv_directory <- "/Users/megansears/Documents/MRMS/correctedSPR/dry"

# Get a list of all CSV files in the directory
csv_files <- list.files(csv_directory, pattern = "\\.csv$", full.names = TRUE)

# Register parallel backend using doParallel
# Replace 'n_cores' with the number of CPU cores you want to utilize
n_cores <- 8  # Adjust the number of cores as needed
cl <- makeCluster(n_cores)
registerDoParallel(cl)

# Function to read CSV files using data.table
read_csv_file <- function(file) {
  dt <- data.table::fread(file, na.strings = c("NA", "NaN", "N/A"))
  
  # Check if the filename ends with "0_0.csv" and the "datetime" column contains only dates
  if (grepl("0_0.csv$", file) && "datetime" %in% colnames(dt) &&
      all(is.na(as.POSIXct(dt$datetime, format = "%Y-%m-%d %H:%M:%S")))) {
    dt$datetime <- paste0(dt$datetime, " 00:00:00")  # Append "00:00:00" to the datetime values
  }
  
  # Convert the "datetime" column to POSIXct format (consistent date-time format)
  dt$datetime <- as.POSIXct(dt$datetime, format = "%m/%d/%y %H:%M:%S")
  
  dt
}

# Use foreach to read all CSV files in parallel
result <- foreach(file = csv_files, .packages = "data.table") %dopar% {
  read_csv_file(file)
}

# Stop the parallel backend
stopCluster(cl)

# Combine the list of data.tables into a single data.table
final_data <- rbindlist(result)

# Check the dimensions of the final_data
print(dim(final_data))

# Write the combined data to a CSV file
write.csv(final_data, '/Users/megansears/Documents/MRMS/correctedSPR/dry/pixel_dry_test.csv')

########################################

dry_mrms <- read_csv('/Users/megansears/Documents/MRMS/correctedSPR/dry/pixel_dry_test.csv') %>%
  mutate(p = p_mmhr*(1/30),
    #datetime = datetime + (7 * 60 * 60),
         p = ifelse(p_mmhr < 0 ,0, p)) %>%
  #filter(datetime > ymd_hms('2022-05-22 23:58:00') &
  #         datetime < ymd_hms('2022-05-24 0:00:00')) %>%
  arrange(datetime) %>%
  mutate(p_cum = cumsum(p))

dry_plot <- dry_mrms %>% 
  #mutate(p = cumsum(p)) %>% 
  ggplot(., aes(datetime, p_cum)) +
  geom_line()

ggplotly(dry_plot)

# sum it hourly 
dry_hour <- dry_mrms %>%
  mutate(AdjustedTime = ceiling_date(datetime, "hour")) %>%
  group_by(AdjustedTime) %>%
  summarize(p_mm = sum(p))


```

## another test

```{r}

qpe <- ras('/Users/megansears/Documents/MRMS/MultiSensor/2022/MultiSensor_QPE_01H_Pass2_00.00_20220814-210000.grib2') %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
qpe <- terra::crop(qpe, ext(bbox))

extract <- terra::extract(qpe, dry_rain, touches = T, exact = T)

#mapview(ape) + mapview(bennsheds)

```

