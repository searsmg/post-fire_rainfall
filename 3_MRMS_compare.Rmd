---
title: "misc_mrms_extract"
author: "Megan Sears"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# NOT USING YET

## TB PIXEL ANAYLSIS

### 2021 

```{r extract tb pixels 2021}

# # unzip all grib2.gz (only do this once) **ALREADY COMPLETE
# zip <- list.files(path = '/Users/megansears/Documents/MRMS/QPE/2021', pattern = '.gz',
#                   full.names = T)
# 
# #unzip all listed files
# unzip <- lapply(zip, gunzip)

setwd('/Users/megansears/Documents/MRMS/MultiSensor/2021')

filenames <- list.files(".", pattern='.grib2', full.names=F)   

  for(fileName in filenames) {

#pull in raster
r <- rast(fileName) %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
crop <- terra::crop(r, ext(bbox))

names(crop) <- 'p_mmhr'

#extract using terra bc it has more functions :)
extract <- terra::extract(crop, rain)
extract$name <- rain$site

timestamp <- substr(fileName, 33, 47)

extract <- extract %>%
  mutate(datetime = ymd_hms(timestamp)) %>%
  mutate(datetime = datetime - (7 * 60 * 60)) %>%
  mutate(doy = yday(datetime)) %>%
  mutate(hour = hour(datetime))

write.csv(extract, paste0('extract_', extract$doy[1],'_', extract$hour[1],'.csv'))

  }

```

```{r get pixel csv 2021}

#compile all CSVs
# pixel_21 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2021', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# pixel_22 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2022', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# #remove random columns
# pixel_21 <- pixel_21 %>%
#   dplyr::select(-c(...1, ID))
# 
# pixel_22 <- pixel_22 %>%
#   dplyr::select(-c(...1, ID, doy, hour))
# 
# write_csv(pixel_21, 'mrms_pixel_21.csv')
# write_csv(pixel_22, 'mrms_pixel_22.csv')

```

## 2022

```{r extract tb pixels 2022}

setwd('/Users/megansears/Documents/MRMS/MultiSensor/2022')

filenames <- list.files(".", pattern='.grib2', full.names=F)   

  for(fileName in filenames) {

#pull in raster
r <- rast(fileName) %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
crop <- terra::crop(r, ext(bbox))

names(crop) <- 'p_mmhr'

#extract using terra bc it has more functions :)
extract <- terra::extract(crop, rain)
extract$name <- rain$site

timestamp <- substr(fileName, 33, 47)

extract <- extract %>%
  mutate(datetime = ymd_hms(timestamp)) %>%
  mutate(datetime = datetime - (7 * 60 * 60)) %>%
  mutate(doy = yday(datetime)) %>%
  mutate(hour = hour(datetime))

write.csv(extract, paste0('extract_', extract$doy[1],'_', extract$hour[1],'.csv'))

  }

```

```{r get pixel csv 2022}

#compile all CSVs
# pixel_21 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2021', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# pixel_22 <- list.files(path='/Users/megansears/Documents/MRMS/etf_pixel_2022', full.names = T) %>%
#   lapply(read_csv) %>%
#   bind_rows
# 
# #remove random columns
# pixel_21 <- pixel_21 %>%
#   dplyr::select(-c(...1, ID))
# 
# pixel_22 <- pixel_22 %>%
#   dplyr::select(-c(...1, ID, doy, hour))
# 
# write_csv(pixel_21, 'mrms_pixel_21.csv')
# write_csv(pixel_22, 'mrms_pixel_22.csv')

```

## WATERSHED MRMS

### Pull watershed MRMS
Using the multi sensor QPE 

```{r}
# do ET first 
setwd('/Users/megansears/Documents/MRMS/MultiSensor/2022')

filenames <- list.files(".", pattern='.grib2', full.names=F)   

  for(fileName in filenames) {

#pull in raster
r <- rast(fileName) %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
crop <- terra::crop(r, ext(bbox))

names(crop) <- 'p_mmhr'

#extract using terra bc it has more functions :)
extract <- terra::extract(crop, et_sheds, fun = mean, touches = T, exact = T)
extract$name <- et_sheds$Name

timestamp <- substr(fileName, 33, 47)

extract <- extract %>%
  mutate(datetime = ymd_hms(timestamp)) %>%
  mutate(datetime = datetime - (7 * 60 * 60)) %>%
  mutate(doy = yday(datetime)) %>%
  mutate(hour = hour(datetime))

write.csv(extract, paste0('extract_', extract$doy[1],'_', extract$hour[1],'.csv'))

  }

```

```{r get watershed csv 2021}

#compile all CSVs
etf_mrms_21 <- list.files(path='/Users/megansears/Documents/MRMS/MultiSensor/etf_2021', full.names = T) %>%
  lapply(read_csv) %>%
  bind_rows

etf_mrms_22 <- list.files(path='/Users/megansears/Documents/MRMS/MultiSensor/etf_2022', full.names = T) %>%
  lapply(read_csv) %>%
  bind_rows

#remove random columns
etf_mrms_21 <- etf_mrms_21 %>%
  dplyr::select(-c(...1, ID))

etf_mrms_22 <- etf_mrms_22 %>%
  dplyr::select(-c(...1, ID, doy, hour))

write_csv(etf_mrms_21, 'etf_mrms_21.csv')
write_csv(etf_mrms_22, 'etf_mrms_22.csv')

```

# MISC CODE

## doing a test of Dry Creek pixel for summer 2022

```{r}

dry_rain <- rain %>%
  filter(site == 'dry')

library(foreach)
library(doParallel)

# Set the number of cores for parallel processing
num_cores <- 8

# Set the working directory
setwd("/Users/megansears/Documents/MRMS/correctedSPR")

# Get the list of raster filenames
filenames <- list.files(".", pattern = ".tif", full.names = FALSE)

# Register parallel backend with the specified number of cores
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Define the output directory
output_dir <- "/Users/megansears/Documents/MRMS/correctedSPR/dry"

# Parallelize the loop using foreach
foreach(fileName = filenames, .packages = c("raster", "lubridate", "dplyr")) %dopar% {
  # Pull in raster
  r <- terra::rast(fileName)
  names(r) <- "p_mmhr"
  
  # Extract using terra package
  extract <- terra::extract(r, dry_rain)
  extract$name <- dry_rain$site
  
  timestamp <- substr(fileName, 12, 26)
  
  # Manipulate the extracted data
  extract <- extract %>%
    mutate(datetime = ymd_hms(timestamp)) %>%
    # mutate(datetime = datetime - (7 * 60 * 60)) %>%
    mutate(doy = yday(datetime)) %>%
    mutate(hour = hour(datetime),
           min = minute(datetime))
  
  # Generate the output filename
  output_filename <- paste0(output_dir, "extract_", extract$doy[1], "_", extract$hour[1], "_", extract$min[1], ".csv")
  
  # Write the extracted data to a CSV file
  write.csv(extract, file = output_filename)
  
  # Return a message indicating completion
  cat("Data extraction completed for:", fileName, "\n")
}

# Stop the parallel backend
stopCluster(cl)

##############################
# move the csvs into another folder
# Set the source and destination directory paths
source_dir <- '/Users/megansears/Documents/MRMS/correctedSPR'
destination_dir <- '/Users/megansears/Documents/MRMS/correctedSPR/dry'

# Get the list of files in the source directory that have 2022
file_list <- list.files(source_dir, pattern = '.csv')

# Move each file to the destination directory
for (file in file_list) {
  source_path <- file.path(source_dir, file)
  destination_path <- file.path(destination_dir, file)
  
  # Copy the file to the destination directory
  file.copy(source_path, destination_path)
  
  # Remove the original file from the source directory
  file.remove(source_path)
}

test<- read_csv('/Users/megansears/Documents/MRMS/correctedSPR/dry/dryextract_122_0_14.csv')

###########################################

# Define the directory where the CSV files are located
csv_directory <- "/Users/megansears/Documents/MRMS/correctedSPR/dry"

# Get a list of all CSV files in the directory
csv_files <- list.files(csv_directory, pattern = "\\.csv$", full.names = TRUE)

# Register parallel backend using doParallel
# Replace 'n_cores' with the number of CPU cores you want to utilize
n_cores <- 8  # Adjust the number of cores as needed
cl <- makeCluster(n_cores)
registerDoParallel(cl)

# Function to read CSV files using data.table
read_csv_file <- function(file) {
  dt <- data.table::fread(file, na.strings = c("NA", "NaN", "N/A"))
  
  # Check if the filename ends with "0_0.csv" and the "datetime" column contains only dates
  if (grepl("0_0.csv$", file) && "datetime" %in% colnames(dt) &&
      all(is.na(as.POSIXct(dt$datetime, format = "%Y-%m-%d %H:%M:%S")))) {
    dt$datetime <- paste0(dt$datetime, " 00:00:00")  # Append "00:00:00" to the datetime values
  }
  
  # Convert the "datetime" column to POSIXct format (consistent date-time format)
  dt$datetime <- as.POSIXct(dt$datetime, format = "%m/%d/%y %H:%M:%S")
  
  dt
}

# Use foreach to read all CSV files in parallel
result <- foreach(file = csv_files, .packages = "data.table") %dopar% {
  read_csv_file(file)
}

# Stop the parallel backend
stopCluster(cl)

# Combine the list of data.tables into a single data.table
final_data <- rbindlist(result)

# Check the dimensions of the final_data
print(dim(final_data))

# Write the combined data to a CSV file
write.csv(final_data, '/Users/megansears/Documents/MRMS/correctedSPR/dry/pixel_dry_test.csv')

########################################

dry_mrms <- read_csv('/Users/megansears/Documents/MRMS/correctedSPR/dry/pixel_dry_test.csv') %>%
  mutate(p = p_mmhr*(1/30),
    #datetime = datetime + (7 * 60 * 60),
         p = ifelse(p_mmhr < 0 ,0, p)) %>%
  #filter(datetime > ymd_hms('2022-05-22 23:58:00') &
  #         datetime < ymd_hms('2022-05-24 0:00:00')) %>%
  arrange(datetime) %>%
  mutate(p_cum = cumsum(p))

dry_plot <- dry_mrms %>% 
  #mutate(p = cumsum(p)) %>% 
  ggplot(., aes(datetime, p_cum)) +
  geom_line()

ggplotly(dry_plot)

# sum it hourly 
dry_hour <- dry_mrms %>%
  mutate(AdjustedTime = ceiling_date(datetime, "hour")) %>%
  group_by(AdjustedTime) %>%
  summarize(p_mm = sum(p))


```

## another test

```{r}

qpe <- ras('/Users/megansears/Documents/MRMS/MultiSensor/2022/MultiSensor_QPE_01H_Pass2_00.00_20220814-210000.grib2') %>%
  terra::project(., 'EPSG:26913')

#crop raster to cpf
qpe <- terra::crop(qpe, ext(bbox))

extract <- terra::extract(qpe, dry_rain, touches = T, exact = T)

#mapview(ape) + mapview(bennsheds)

```
